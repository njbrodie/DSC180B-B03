{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2c730d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv2D, Flatten, Input, Reshape, AveragePooling2D, MaxPooling2D, Conv2DTranspose, TimeDistributed, LSTM, GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "58fdcf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations = ['ssp126', 'ssp370', 'ssp585', 'hist-GHG', 'hist-aer']\n",
    "len_historical = 165\n",
    "data_path = os.getcwd() + \"/\"\n",
    "def normalize(data, var, meanstd_dict):\n",
    "    mean = meanstd_dict[var][0]\n",
    "    std = meanstd_dict[var][1]\n",
    "    return (data - mean)/std\n",
    "\n",
    "def unnormalize(data, var, meanstd_dict):\n",
    "    mean = meanstd_dict[var][0]\n",
    "    std = meanstd_dict[var][1]\n",
    "    return data * std + mean\n",
    "slider = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd1a1145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for reshaping the data \n",
    "def input_for_training(X_train_xr, skip_historical=False, len_historical=None): \n",
    "    \n",
    "    X_train_np =  X_train_xr.to_array().transpose('time', 'latitude', 'longitude', 'variable').data\n",
    "\n",
    "    time_length = X_train_np.shape[0]\n",
    "    # If we skip historical data, the first sequence created has as last element the first scenario data point\n",
    "    if skip_historical:\n",
    "        X_train_to_return = np.array([X_train_np[i:i+slider] for i in range(len_historical-slider+1, time_length-slider+1)])\n",
    "    # Else we just go through the whole dataset historical + scenario (does not matter in the case of 'hist-GHG' and 'hist_aer')\n",
    "    else:\n",
    "        X_train_to_return = np.array([X_train_np[i:i+slider] for i in range(0, time_length-slider+1)])\n",
    "    \n",
    "    return X_train_to_return \n",
    "\n",
    "\n",
    "def output_for_training(Y_train_xr, var, skip_historical=False, len_historical=None): \n",
    "    Y_train_np = Y_train_xr[var].data\n",
    "    \n",
    "    time_length = Y_train_np.shape[0]\n",
    "    \n",
    "    # If we skip historical data, the first sequence created has as target element the first scenario data point\n",
    "    if skip_historical:\n",
    "        Y_train_to_return = np.array([[Y_train_np[i+slider-1]] for i in range(len_historical-slider+1, time_length-slider+1)])\n",
    "    # Else we just go through the whole dataset historical + scenario (does not matter in the case of 'hist-GHG' and 'hist_aer')\n",
    "    else:\n",
    "        Y_train_to_return = np.array([[Y_train_np[i+slider-1]] for i in range(0, time_length-slider+1)])\n",
    "    \n",
    "    return Y_train_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "94aed467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_data(simulations):\n",
    "    data_path = os.getcwd() + \"/\"\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "\n",
    "    for i, sim in enumerate(simulations):\n",
    "\n",
    "        input_name = 'inputs_' + sim + '.nc'\n",
    "        output_name = 'outputs_' + sim + '.nc'\n",
    "\n",
    "        # Just load hist data in these cases 'hist-GHG' and 'hist-aer'\n",
    "        if 'hist' in sim:\n",
    "            # load inputs \n",
    "            input_xr = xr.open_dataset(data_path + input_name)\n",
    "\n",
    "            # load outputs                                                             \n",
    "            output_xr = xr.open_dataset(data_path + output_name).mean(dim='member')\n",
    "            output_xr = output_xr.assign({\"pr\": output_xr.pr * 86400,\n",
    "                                          \"pr90\": output_xr.pr90 * 86400}).rename({'lon':'longitude', \n",
    "                                                                                   'lat': 'latitude'}).transpose('time','latitude', 'longitude').drop(['quantile'])\n",
    "\n",
    "        # Concatenate with historical data in the case of scenario 'ssp126', 'ssp370' and 'ssp585'\n",
    "        else:\n",
    "            # load inputs \n",
    "            input_xr = xr.open_mfdataset([data_path + 'inputs_historical.nc', \n",
    "                                        data_path + input_name]).compute()\n",
    "\n",
    "            # load outputs                                                             \n",
    "            output_xr = xr.concat([xr.open_dataset(data_path + 'outputs_historical.nc').mean(dim='member'),\n",
    "                                   xr.open_dataset(data_path + output_name).mean(dim='member')],\n",
    "                                   dim='time').compute()\n",
    "            output_xr = output_xr.assign({\"pr\": output_xr.pr * 86400,\n",
    "                                          \"pr90\": output_xr.pr90 * 86400}).rename({'lon':'longitude', \n",
    "                                                                                   'lat': 'latitude'}).transpose('time','latitude', 'longitude').drop(['quantile'])\n",
    "\n",
    "        print(input_xr.dims, sim)\n",
    "\n",
    "        # Append to list \n",
    "        X_train.append(input_xr)\n",
    "        Y_train.append(output_xr)\n",
    "        \n",
    "    # Compute mean/std of each variable for the whole dataset\n",
    "    meanstd_inputs = {}\n",
    "\n",
    "    for var in ['CO2', 'CH4', 'SO2', 'BC']:\n",
    "        # To not take the historical data into account several time we have to slice the scenario datasets\n",
    "        # and only keep the historical data once (in the first ssp index 0 in the simus list)\n",
    "        array = np.concatenate([X_train[i][var].data for i in [0, 3, 4]] + \n",
    "                               [X_train[i][var].sel(time=slice(len_historical, None)).data for i in range(1, 3)])\n",
    "        print((array.mean(), array.std()))\n",
    "        meanstd_inputs[var] = (array.mean(), array.std())\n",
    "        \n",
    "    # normalize input data \n",
    "    X_train_norm = [] \n",
    "    for i, train_xr in enumerate(X_train): \n",
    "        for var in ['CO2', 'CH4', 'SO2', 'BC']: \n",
    "            var_dims = train_xr[var].dims\n",
    "            train_xr=train_xr.assign({var: (var_dims, normalize(train_xr[var].data, var, meanstd_inputs))}) \n",
    "        X_train_norm.append(train_xr)\n",
    "\n",
    "    X_test = xr.open_mfdataset([data_path + 'inputs_historical.nc',\n",
    "                                data_path + 'inputs_ssp245.nc']).compute()\n",
    "\n",
    "    for var in ['CO2', 'CH4', 'SO2', 'BC']: \n",
    "        var_dims = X_test[var].dims\n",
    "        X_test = X_test.assign({var: (var_dims, normalize(X_test[var].data, var, meanstd_inputs))})\n",
    "\n",
    "    X_test_np = input_for_training(X_test, skip_historical=False, len_historical=len_historical)\n",
    "    \n",
    "    return X_train_norm, Y_train, X_test_np, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "97ca7602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen({'time': 251, 'longitude': 144, 'latitude': 96}) ssp126\n",
      "Frozen({'time': 251, 'longitude': 144, 'latitude': 96}) ssp370\n",
      "Frozen({'time': 251, 'longitude': 144, 'latitude': 96}) ssp585\n",
      "Frozen({'time': 165, 'longitude': 144, 'latitude': 96}) hist-GHG\n",
      "Frozen({'time': 165, 'longitude': 144, 'latitude': 96}) hist-aer\n",
      "(1074.172303244536, 1755.690699230666)\n",
      "(0.1927369743762821, 0.18457590641432994)\n",
      "(2.5623359997066755e-12, 2.250114566783271e-11)\n",
      "(1.4947905009818064e-13, 1.0313342554838387e-12)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test_np, X_test = get_input_data(simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "40c71f9b-a71d-4057-bd1b-30c9d11ae61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<xarray.Dataset>\n",
       " Dimensions:                    (time: 251, latitude: 96, longitude: 144)\n",
       " Coordinates:\n",
       "   * time                       (time) int64 1850 1851 1852 ... 2098 2099 2100\n",
       "   * latitude                   (latitude) float64 -90.0 -88.11 ... 88.11 90.0\n",
       "   * longitude                  (longitude) float64 0.0 2.5 5.0 ... 355.0 357.5\n",
       " Data variables:\n",
       "     diurnal_temperature_range  (time, latitude, longitude) float32 0.03264 .....\n",
       "     tas                        (time, latitude, longitude) float32 0.32 ... 3...\n",
       "     pr                         (time, latitude, longitude) float64 0.02337 .....\n",
       "     pr90                       (time, latitude, longitude) float64 0.02468 .....,\n",
       " <xarray.Dataset>\n",
       " Dimensions:                    (time: 251, latitude: 96, longitude: 144)\n",
       " Coordinates:\n",
       "   * time                       (time) int64 1850 1851 1852 ... 2098 2099 2100\n",
       "   * latitude                   (latitude) float64 -90.0 -88.11 ... 88.11 90.0\n",
       "   * longitude                  (longitude) float64 0.0 2.5 5.0 ... 355.0 357.5\n",
       " Data variables:\n",
       "     diurnal_temperature_range  (time, latitude, longitude) float32 0.03264 .....\n",
       "     tas                        (time, latitude, longitude) float32 0.32 ... 8...\n",
       "     pr                         (time, latitude, longitude) float64 0.02337 .....\n",
       "     pr90                       (time, latitude, longitude) float64 0.02468 .....,\n",
       " <xarray.Dataset>\n",
       " Dimensions:                    (time: 251, latitude: 96, longitude: 144)\n",
       " Coordinates:\n",
       "   * time                       (time) int64 1850 1851 1852 ... 2098 2099 2100\n",
       "   * latitude                   (latitude) float64 -90.0 -88.11 ... 88.11 90.0\n",
       "   * longitude                  (longitude) float64 0.0 2.5 5.0 ... 355.0 357.5\n",
       " Data variables:\n",
       "     diurnal_temperature_range  (time, latitude, longitude) float32 0.03264 .....\n",
       "     tas                        (time, latitude, longitude) float32 0.32 ... 1...\n",
       "     pr                         (time, latitude, longitude) float64 0.02337 .....\n",
       "     pr90                       (time, latitude, longitude) float64 0.02468 .....,\n",
       " <xarray.Dataset>\n",
       " Dimensions:                    (latitude: 96, longitude: 144, time: 165)\n",
       " Coordinates:\n",
       "   * latitude                   (latitude) float64 -90.0 -88.11 ... 88.11 90.0\n",
       "   * longitude                  (longitude) float64 0.0 2.5 5.0 ... 355.0 357.5\n",
       "   * time                       (time) int64 1850 1851 1852 ... 2012 2013 2014\n",
       " Data variables:\n",
       "     diurnal_temperature_range  (time, latitude, longitude) float32 -0.04071 ....\n",
       "     tas                        (time, latitude, longitude) float32 0.2223 ......\n",
       "     pr                         (time, latitude, longitude) float64 -0.01905 ....\n",
       "     pr90                       (time, latitude, longitude) float64 -0.05889 ....,\n",
       " <xarray.Dataset>\n",
       " Dimensions:                    (latitude: 96, longitude: 144, time: 165)\n",
       " Coordinates:\n",
       "   * latitude                   (latitude) float64 -90.0 -88.11 ... 88.11 90.0\n",
       "   * longitude                  (longitude) float64 0.0 2.5 5.0 ... 355.0 357.5\n",
       "   * time                       (time) int64 1850 1851 1852 ... 2012 2013 2014\n",
       " Data variables:\n",
       "     diurnal_temperature_range  (time, latitude, longitude) float32 0.07284 .....\n",
       "     tas                        (time, latitude, longitude) float32 -0.142 ......\n",
       "     pr                         (time, latitude, longitude) float64 0.009859 ....\n",
       "     pr90                       (time, latitude, longitude) float64 0.02055 .....]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72808456",
   "metadata": {},
   "source": [
    "## Train and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3dfc8e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train_norm, Y_train, var_to_predict, X_test_np, X_test):\n",
    "    \n",
    "    # skip_historical set to (i < 2) because of the order of the scenario and historical runs in the X_train and Y_train lists.\n",
    "    # In details: ssp126 0, ssp370 1 = skip historical part of the data, ssp585 2, hist-GHG 3 and hist-aer 4 = keep the whole sequence\n",
    "    X_train_all = np.concatenate([input_for_training(X_train_norm[i], skip_historical=(i<2), len_historical=len_historical) for i in range(len(simulations))], axis = 0)\n",
    "    Y_train_all = np.concatenate([output_for_training(Y_train[i], var_to_predict, skip_historical=(i<2), len_historical=len_historical) for i in range(len(simulations))], axis=0)\n",
    "    print(X_train_all.shape)\n",
    "    print(Y_train_all.shape)\n",
    "    \n",
    "    \n",
    "    seed = 6 \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    keras.backend.clear_session()\n",
    "    cnn_model = None\n",
    "    \n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Input(shape=(slider, 96, 144, 4)))\n",
    "    cnn_model.add(TimeDistributed(Conv2D(20, (3, 3), padding='same', activation='relu'), input_shape=(slider, 96, 144, 4)))\n",
    "    cnn_model.add(TimeDistributed(AveragePooling2D(2)))\n",
    "    cnn_model.add(TimeDistributed(GlobalAveragePooling2D()))\n",
    "    cnn_model.add(LSTM(25, activation='relu'))\n",
    "    cnn_model.add(Dense(1*96*144))\n",
    "    cnn_model.add(Activation('linear'))\n",
    "    cnn_model.add(Reshape((1, 96, 144)))\n",
    "    \n",
    "    cnn_model.compile(optimizer='rmsprop', loss='mse', metrics=['mse'])\n",
    "    \n",
    "    hist = cnn_model.fit(X_train_all,\\\n",
    "                         Y_train_all,\\\n",
    "                         use_multiprocessing=True,\\\n",
    "                         #workers=5,\\\n",
    "                         batch_size=16,\\\n",
    "                         epochs=30,\\\n",
    "                         verbose=1)\n",
    "\n",
    "    # Make predictions using trained model \n",
    "    m_pred = cnn_model.predict(X_test_np)\n",
    "    # Reshape to xarray \n",
    "    m_pred = m_pred.reshape(m_pred.shape[0], m_pred.shape[2], m_pred.shape[3])\n",
    "    m_pred = xr.DataArray(m_pred, dims=['time', 'lat', 'lon'], coords=[X_test.time.data[slider-1:], X_test.latitude.data, X_test.longitude.data])\n",
    "    xr_prediction = m_pred.transpose('lat', 'lon', 'time').sel(time=slice(2015,2101)).to_dataset(name=var_to_predict)\n",
    "\n",
    "    if var_to_predict==\"pr90\" or var_to_predict==\"pr\":\n",
    "        xr_prediction = xr_prediction.assign({var_to_predict: xr_prediction[var_to_predict] / 86400})\n",
    "\n",
    "    # Save test predictions as .nc \n",
    "    if var_to_predict == 'diurnal_temperature_range':\n",
    "        xr_prediction.to_netcdf('CNN_outputs_ssp245_predict_dtr.nc', 'w')\n",
    "    else:\n",
    "        xr_prediction.to_netcdf('CNN_outputs_ssp245_predict_{}.nc'.format(var_to_predict), 'w')\n",
    "    xr_prediction.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "11f6d7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(726, 10, 96, 144, 4)\n",
      "(726, 1, 96, 144)\n",
      "Epoch 1/30\n",
      "46/46 [==============================] - 6s 106ms/step - loss: 1.8917 - mse: 1.8917\n",
      "Epoch 2/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.4632 - mse: 0.4632\n",
      "Epoch 3/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.3945 - mse: 0.3945\n",
      "Epoch 4/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.3874 - mse: 0.3874\n",
      "Epoch 5/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.3794 - mse: 0.3794\n",
      "Epoch 6/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.3745 - mse: 0.3745\n",
      "Epoch 7/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.3654 - mse: 0.3654\n",
      "Epoch 8/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.3486 - mse: 0.3486\n",
      "Epoch 9/30\n",
      "46/46 [==============================] - 5s 109ms/step - loss: 0.3196 - mse: 0.3196\n",
      "Epoch 10/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.3113 - mse: 0.3113\n",
      "Epoch 11/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.3051 - mse: 0.3051\n",
      "Epoch 12/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.3050 - mse: 0.3050\n",
      "Epoch 13/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.3063 - mse: 0.3063\n",
      "Epoch 14/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.3001 - mse: 0.3001\n",
      "Epoch 15/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2993 - mse: 0.2993\n",
      "Epoch 16/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.2980 - mse: 0.2980\n",
      "Epoch 17/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.2983 - mse: 0.2983\n",
      "Epoch 18/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2980 - mse: 0.2980\n",
      "Epoch 19/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2973 - mse: 0.2973\n",
      "Epoch 20/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.2973 - mse: 0.2973\n",
      "Epoch 21/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2941 - mse: 0.2941\n",
      "Epoch 22/30\n",
      "46/46 [==============================] - 5s 110ms/step - loss: 0.2945 - mse: 0.2945\n",
      "Epoch 23/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.2936 - mse: 0.2936\n",
      "Epoch 24/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.2955 - mse: 0.2955\n",
      "Epoch 25/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2949 - mse: 0.2949\n",
      "Epoch 26/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2950 - mse: 0.2950\n",
      "Epoch 27/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2914 - mse: 0.2914\n",
      "Epoch 28/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2930 - mse: 0.2930\n",
      "Epoch 29/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.2917 - mse: 0.2917\n",
      "Epoch 30/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2886 - mse: 0.2886\n",
      "8/8 [==============================] - 1s 43ms/step\n",
      "(726, 10, 96, 144, 4)\n",
      "(726, 1, 96, 144)\n",
      "Epoch 1/30\n",
      "46/46 [==============================] - 6s 105ms/step - loss: 0.0771 - mse: 0.0771\n",
      "Epoch 2/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.0762 - mse: 0.0762\n",
      "Epoch 3/30\n",
      "46/46 [==============================] - 5s 109ms/step - loss: 0.0647 - mse: 0.0647\n",
      "Epoch 4/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 0.0423 - mse: 0.0423\n",
      "Epoch 5/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.0397 - mse: 0.0397\n",
      "Epoch 6/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 0.0393 - mse: 0.0393\n",
      "Epoch 7/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.0390 - mse: 0.0390\n",
      "Epoch 8/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.0388 - mse: 0.0388\n",
      "Epoch 9/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.0386 - mse: 0.0386\n",
      "Epoch 10/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 0.0384 - mse: 0.0384\n",
      "Epoch 11/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.0381 - mse: 0.0381\n",
      "Epoch 12/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.0378 - mse: 0.0378\n",
      "Epoch 13/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.0375 - mse: 0.0375\n",
      "Epoch 14/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.0372 - mse: 0.0372\n",
      "Epoch 15/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.0370 - mse: 0.0370\n",
      "Epoch 16/30\n",
      "46/46 [==============================] - 5s 109ms/step - loss: 0.0367 - mse: 0.0367\n",
      "Epoch 17/30\n",
      "46/46 [==============================] - 5s 107ms/step - loss: 0.0365 - mse: 0.0365\n",
      "Epoch 18/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 0.0363 - mse: 0.0363\n",
      "Epoch 19/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.0363 - mse: 0.0363\n",
      "Epoch 20/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.0362 - mse: 0.0362\n",
      "Epoch 21/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.0361 - mse: 0.0361\n",
      "Epoch 22/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.0360 - mse: 0.0360\n",
      "Epoch 23/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.0359 - mse: 0.0359\n",
      "Epoch 24/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 0.0358 - mse: 0.0358\n",
      "Epoch 25/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 0.0357 - mse: 0.0357\n",
      "Epoch 26/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.0356 - mse: 0.0356\n",
      "Epoch 27/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 0.0355 - mse: 0.0355\n",
      "Epoch 28/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 0.0353 - mse: 0.0353\n",
      "Epoch 29/30\n",
      "46/46 [==============================] - 5s 109ms/step - loss: 0.0353 - mse: 0.0353\n",
      "Epoch 30/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.0351 - mse: 0.0351\n",
      "8/8 [==============================] - 0s 41ms/step\n",
      "(726, 10, 96, 144, 4)\n",
      "(726, 1, 96, 144)\n",
      "Epoch 1/30\n",
      "46/46 [==============================] - 6s 105ms/step - loss: 0.3230 - mse: 0.3230\n",
      "Epoch 2/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.3216 - mse: 0.3216\n",
      "Epoch 3/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.3017 - mse: 0.3017\n",
      "Epoch 4/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.2839 - mse: 0.2839\n",
      "Epoch 5/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2812 - mse: 0.2812\n",
      "Epoch 6/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2804 - mse: 0.2804\n",
      "Epoch 7/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2802 - mse: 0.2802\n",
      "Epoch 8/30\n",
      "46/46 [==============================] - 5s 103ms/step - loss: 0.2794 - mse: 0.2794\n",
      "Epoch 9/30\n",
      "46/46 [==============================] - 5s 106ms/step - loss: 0.2788 - mse: 0.2788\n",
      "Epoch 10/30\n",
      "46/46 [==============================] - 5s 109ms/step - loss: 0.2793 - mse: 0.2793\n",
      "Epoch 11/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2788 - mse: 0.2788\n",
      "Epoch 12/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2786 - mse: 0.2786\n",
      "Epoch 13/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2784 - mse: 0.2784\n",
      "Epoch 14/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2781 - mse: 0.2781\n",
      "Epoch 15/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2778 - mse: 0.2778\n",
      "Epoch 16/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 0.2769 - mse: 0.2769\n",
      "Epoch 17/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2760 - mse: 0.2760\n",
      "Epoch 18/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 0.2739 - mse: 0.2739\n",
      "Epoch 19/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2719 - mse: 0.2719\n",
      "Epoch 20/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2695 - mse: 0.2695\n",
      "Epoch 21/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 0.2687 - mse: 0.2687\n",
      "Epoch 22/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2678 - mse: 0.2678\n",
      "Epoch 23/30\n",
      "46/46 [==============================] - 5s 109ms/step - loss: 0.2673 - mse: 0.2673\n",
      "Epoch 24/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2669 - mse: 0.2669\n",
      "Epoch 25/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2666 - mse: 0.2666\n",
      "Epoch 26/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 0.2666 - mse: 0.2666\n",
      "Epoch 27/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2663 - mse: 0.2663\n",
      "Epoch 28/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 0.2657 - mse: 0.2657\n",
      "Epoch 29/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2664 - mse: 0.2664\n",
      "Epoch 30/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 0.2656 - mse: 0.2656\n",
      "8/8 [==============================] - 0s 38ms/step\n",
      "(726, 10, 96, 144, 4)\n",
      "(726, 1, 96, 144)\n",
      "Epoch 1/30\n",
      "46/46 [==============================] - 6s 106ms/step - loss: 2.7512 - mse: 2.7512\n",
      "Epoch 2/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 2.5113 - mse: 2.5113\n",
      "Epoch 3/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 2.4460 - mse: 2.4460\n",
      "Epoch 4/30\n",
      "46/46 [==============================] - 5s 109ms/step - loss: 2.4312 - mse: 2.4312\n",
      "Epoch 5/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 2.4153 - mse: 2.4153\n",
      "Epoch 6/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 2.3920 - mse: 2.3920\n",
      "Epoch 7/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 2.3775 - mse: 2.3775\n",
      "Epoch 8/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 2.3589 - mse: 2.3589\n",
      "Epoch 9/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 2.3450 - mse: 2.3450\n",
      "Epoch 10/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 2.3482 - mse: 2.3482\n",
      "Epoch 11/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 2.3374 - mse: 2.3374\n",
      "Epoch 12/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 2.3332 - mse: 2.3332\n",
      "Epoch 13/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 2.3282 - mse: 2.3282\n",
      "Epoch 14/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 2.3217 - mse: 2.3217\n",
      "Epoch 15/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 2.3221 - mse: 2.3221\n",
      "Epoch 16/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 2.3170 - mse: 2.3170\n",
      "Epoch 17/30\n",
      "46/46 [==============================] - 5s 108ms/step - loss: 2.3142 - mse: 2.3142\n",
      "Epoch 18/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 2.3092 - mse: 2.3092\n",
      "Epoch 19/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 2.3144 - mse: 2.3144\n",
      "Epoch 20/30\n",
      "46/46 [==============================] - 5s 103ms/step - loss: 2.3104 - mse: 2.3104\n",
      "Epoch 21/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 2.3107 - mse: 2.3107\n",
      "Epoch 22/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 2.3090 - mse: 2.3090\n",
      "Epoch 23/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 2.3082 - mse: 2.3082\n",
      "Epoch 24/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 2.3043 - mse: 2.3043\n",
      "Epoch 25/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 2.3035 - mse: 2.3035\n",
      "Epoch 26/30\n",
      "46/46 [==============================] - 5s 103ms/step - loss: 2.3041 - mse: 2.3041\n",
      "Epoch 27/30\n",
      "46/46 [==============================] - 5s 103ms/step - loss: 2.3020 - mse: 2.3020\n",
      "Epoch 28/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 2.2985 - mse: 2.2985\n",
      "Epoch 29/30\n",
      "46/46 [==============================] - 5s 105ms/step - loss: 2.3049 - mse: 2.3049\n",
      "Epoch 30/30\n",
      "46/46 [==============================] - 5s 104ms/step - loss: 2.2973 - mse: 2.2973\n",
      "8/8 [==============================] - 1s 43ms/step\n"
     ]
    }
   ],
   "source": [
    "predict_vars = ['tas', 'diurnal_temperature_range', 'pr', 'pr90']\n",
    "for v in predict_vars:\n",
    "    model(X_train, y_train, v, X_test_np, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce8b6a2-1bf3-46f1-99d8-6e3cbbd69bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db137ba1-f9e7-47ed-b57f-10e7d169c5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985384ba-afda-4585-9154-2ace93b688ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
