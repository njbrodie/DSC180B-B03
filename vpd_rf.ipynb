{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b52783-a183-4bf8-81be-b62c20f57f6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from eofs.xarray import Eof\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab55fe11-c186-4f51-8441-121301aa4b64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utilities for normalizing the emissions data\n",
    "min_co2 = 0.\n",
    "max_co2 = 2400\n",
    "def normalize_co2(data):\n",
    "    return data / max_co2\n",
    "\n",
    "min_ch4 = 0.\n",
    "max_ch4 = 0.6\n",
    "def normalize_ch4(data):\n",
    "    return data / max_ch4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37af6a7d-d376-4bd3-a6b8-b12d5bd8735f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# functions to compute various NRMSEs\n",
    "def get_nrmse_spatial(truth, pred):\n",
    "    weights = np.cos(np.deg2rad(truth.lat))\n",
    "    truth = truth.sel(time=slice(2080,None))\n",
    "    pred = pred[-21:]\n",
    "    truth_total = np.abs(truth.weighted(weights).mean(['lat', 'lon']).data.mean())\n",
    "    rmse_spatial = np.sqrt(((truth - pred).mean('time')**2).weighted(weights).mean(['lat','lon'])).data\n",
    "    return rmse_spatial / truth_total \n",
    "\n",
    "def get_nrmse_global(truth, pred):\n",
    "    weights = np.cos(np.deg2rad(truth.lat))\n",
    "    truth = truth.sel(time=slice(2080,None))\n",
    "    pred = pred[-21:]\n",
    "    truth_total = np.abs(truth.weighted(weights).mean(['lat', 'lon']).data.mean())\n",
    "    rmse_global = np.sqrt((((truth - pred).weighted(weights).mean(['lat', 'lon']))**2).data.mean())\n",
    "    return rmse_global / truth_total \n",
    "\n",
    "def get_nrmse(truth, pred):\n",
    "    return get_nrmse_spatial(truth, pred) + 5 * get_nrmse_global(truth, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f8b03e3-50ca-49f9-b1dd-be2be1368e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# computes t-test for differences\n",
    "def ttest_rel_from_stats(diff_mean, diff_std, diff_num):\n",
    "    z = diff_mean / np.sqrt(diff_std ** 2 / diff_num)\n",
    "    p = distributions.t.sf(np.abs(z), diff_num - 1) * 2\n",
    "    return z, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "294ebbfa-1503-4714-a048-a9ea4197f09a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute vpd, given relative humidity and temperature\n",
    "def get_vpd(humidity_data, tas_data):\n",
    "    svp = 0.6112 * np.exp(17.76*(tas_data-273)/((tas_data-273) + 243.5))\n",
    "    vpd = svp * (1 - humidity_data/100)\n",
    "    return vpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7cf54c6-cc60-40e5-86d4-aa5b12c8d047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_scenarios = ['historical', 'ssp126', 'ssp370', 'ssp585', 'hist-aer', 'hist-GHG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d034dfd8-7495-401e-b09b-fc6cd6846dc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xtrain = xr.concat(\n",
    "    [xr.open_dataset(f'train_val/inputs_{s}.nc') for s in train_scenarios]\n",
    "    , dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58e46a5f-c3c0-441f-9187-000ec1f92beb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load and compute VPD for train and test data\n",
    "hursTrain = xr.concat([xr.open_dataset(f'vpd_data/hurs_{s}.nc') for s in train_scenarios]\n",
    "    , dim='time')\n",
    "tasTrain = xr.concat([xr.open_dataset(f'vpd_data/tas_{s}.nc') for s in train_scenarios]\n",
    "    , dim='time')\n",
    "vpdTrain = get_vpd(hursTrain.hurs, tasTrain.tas).to_dataset(name='vpd')\n",
    "\n",
    "hursTest = xr.open_dataset('vpd_data/hurs_ssp245.nc')\n",
    "tasTest = xr.open_dataset('vpd_data/tas_ssp245.nc')\n",
    "vpdTest = get_vpd(hursTest.hurs, tasTest.tas).to_dataset(name='vpd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00940636-5eb1-4f9f-8e2d-dd96560c0272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add year data to X and Y\n",
    "Xtrain[\"time\"]=np.arange(1, 754)\n",
    "vpdTrain[\"time\"]=np.arange(1, 754)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6070c0b-1f32-4652-bc76-e2c1506d6a1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an EOF solver to do the EOF analysis. Square-root of cosine of\n",
    "# latitude weights are applied before the computation of EOFs.\n",
    "bc_solver = Eof(Xtrain['BC'])\n",
    "so2_solver = Eof(Xtrain['SO2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02555c7d-4f29-4821-8a4c-e609ef562fe3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the leading EOF, expressed as the correlation between the leading\n",
    "# PC time series and the input SST anomalies at each grid point, and the\n",
    "# leading PC time series itself.\n",
    "bc_eofs = bc_solver.eofsAsCorrelation(neofs=5)\n",
    "bc_pcs = bc_solver.pcs(npcs=5, pcscaling=1)\n",
    "\n",
    "so2_eofs = so2_solver.eofsAsCorrelation(neofs=5)\n",
    "so2_pcs = so2_solver.pcs(npcs=5, pcscaling=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce1dd03b-1ab3-4c6a-a850-a44af0e9f5f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the Principle Components of the aerosol emissions (calculated above) in to Pandas DataFrames\n",
    "bc_df = bc_pcs.to_dataframe().unstack('mode')\n",
    "bc_df.columns = [f\"BC_{i}\" for i in range(5)]\n",
    "\n",
    "so2_df = so2_pcs.to_dataframe().unstack('mode')\n",
    "so2_df.columns = [f\"SO2_{i}\" for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87831790-8ee2-49b4-8618-ce3766e886c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bring the emissions data back together again and normalize\n",
    "leading_historical_inputs = pd.DataFrame({\n",
    "    \"CO2\": normalize_co2(Xtrain[\"CO2\"].data),\n",
    "    \"CH4\": normalize_ch4(Xtrain[\"CH4\"].data)\n",
    "}, index=Xtrain[\"CO2\"].coords['time'].data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "114c5d36-2555-4f17-ab4c-ecd800620279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# format Xtrain and ytrain to be used for random forest model\n",
    "Xinput = pd.concat([leading_historical_inputs, bc_df, so2_df], axis=1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0f2475d-8022-4abb-9f03-4eab499f68bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Yinput = vpdTrain['vpd'].stack(dim=['lat','lon']).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fd91528-fefd-4d94-aee1-3c0ea8c4c11b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# taken verbatim from Original_RF_model.ipynb\n",
    "# parameters & hyperparameters for model\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 300, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5,55, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [5, 10, 15, 25]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [4, 8, 12]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b6e11-efe5-438a-99f2-dc67550ca16b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 29 candidates, totalling 87 fits\n"
     ]
    }
   ],
   "source": [
    "# dictionary to hold all 4 random forest models\n",
    "random_forest_regressor = RandomForestRegressor(random_state=0)\n",
    "\n",
    "# create rscvs for each model\n",
    "RSCV = RandomizedSearchCV(\n",
    "        estimator = random_forest_regressor, \n",
    "        param_distributions = random_grid, \n",
    "        n_iter = 29, \n",
    "        cv = 3, \n",
    "        verbose=2, \n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "# create random forests\n",
    "random_forest = RSCV.fit(Xinput, Yinput)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
